{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17d99d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 1: Document Preparation\n",
      "================================================================================\n",
      "Created 5 documents\n",
      "\n",
      "================================================================================\n",
      "STEP 2: Document Chunking\n",
      "================================================================================\n",
      "Split into 10 chunks\n",
      "\n",
      "Chunk 1: Vector Databases and Embeddings:\n",
      "    Vector databases store information as numerical vectors (embedd...\n",
      "\n",
      "Chunk 2: similarity. This enables semantic search - finding relevant information \n",
      "    based on meaning rather...\n",
      "\n",
      "Chunk 3: Retrieval-Augmented Generation (RAG):\n",
      "    RAG enhances Large Language Models by retrieving relevant ...\n",
      "\n",
      "================================================================================\n",
      "STEP 3: Creating Vector Store\n",
      "================================================================================\n",
      "Loading embedding model (this may take a moment)...\n",
      "Vector store created successfully!\n",
      "Total vectors stored: 30\n",
      "\n",
      "================================================================================\n",
      "STEP 4: Testing Retrieval\n",
      "================================================================================\n",
      "\n",
      "Query: 'How does RAG work?'\n",
      "Retrieved 3 documents:\n",
      "\n",
      "1. - Learn from feedback to improve performance\n",
      "    Agents can use RAG to access knowledge dynamically during task execution....\n",
      "   Source: ai_guide_4.txt\n",
      "\n",
      "2. - Learn from feedback to improve performance\n",
      "    Agents can use RAG to access knowledge dynamically during task execution....\n",
      "   Source: ai_guide_4.txt\n",
      "\n",
      "3. - Learn from feedback to improve performance\n",
      "    Agents can use RAG to access knowledge dynamically during task execution....\n",
      "   Source: ai_guide_4.txt\n",
      "\n",
      "================================================================================\n",
      "STEP 5: Building RAG Chain with LCEL\n",
      "================================================================================\n",
      "\n",
      "Prompt template created:\n",
      "You are a helpful AI assistant. Use the following context to answer the question.\n",
      "If you cannot answer based on the context, say so.\n",
      "\n",
      "Context:\n",
      "{context}\n",
      "\n",
      "Question: {question}\n",
      "\n",
      "Answer: Let me help you understand this based on the provided information.\n",
      "\n",
      "✓ RAG chain created successfully using LCEL\n",
      "\n",
      "Chain structure:\n",
      "  Input → {context: retriever → format, question: passthrough}\n",
      "       → Prompt → LLM → Output Parser → Final Answer\n",
      "\n",
      "================================================================================\n",
      "STEP 6: Testing RAG Chain\n",
      "================================================================================\n",
      "\n",
      "Question: What is RAG?\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s7/nglqvbh926vbsjnphgfq6k_h0000gn/T/ipykernel_21032/52428427.py:31: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import ChatOllama``.\n",
      "  return ChatOllama(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Based on the context, it appears that RAG refers to a mechanism or system used by agents (likely artificial intelligence or machine learning models) to access knowledge dynamically during task execution. This allows them to learn from feedback and improve their performance over time.\n",
      "\n",
      "In other words, RAG seems to be a way for intelligent systems to adapt and learn as they interact with their environment, using the information they gather to refine their understanding and behavior.\n",
      "\n",
      "\n",
      "Question: What are the key considerations for chunking?\n",
      "--------------------------------------------------------------------------------\n",
      "Answer: According to the given context, the key considerations for chunking strategies include:\n",
      "\n",
      "1. Chunk size: Smaller chunks (100-500 tokens) for precise retrieval.\n",
      "2. Overlap: 10-20% overlap prevents losing context at boundaries.\n",
      "\n",
      "These two factors are crucial in determining how documents should be split into chunks to fit within LLM (Large Language Model) context windows and improve retrieval precision.\n",
      "\n",
      "\n",
      "Question: What embedding models are mentioned?\n",
      "--------------------------------------------------------------------------------\n",
      "Answer: Two embedding models are mentioned:\n",
      "\n",
      "1. OpenAI text-embedding-ada-002\n",
      "2. Sentence-BERT (all-MiniLM-L6-v2)\n",
      "\n",
      "================================================================================\n",
      "STEP 7: RAG Chain with Source Attribution\n",
      "================================================================================\n",
      "\n",
      "Question: Explain what AI agents can do\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Answer:\n",
      "Based on the context, I can explain that AI agents are autonomous systems that can perform four key functions:\n",
      "\n",
      "1. **Perceive their environment**: They can gather data and information about their surroundings through sensors or other means.\n",
      "2. **Make decisions using reasoning and planning**: They have the ability to analyze data, make logical conclusions, and plan out actions to achieve a goal.\n",
      "3. **Take actions using tools**: They can utilize various tools such as APIs, databases, search engines, etc., to execute their planned actions.\n",
      "4. **Learn from feedback to improve performance**: They can learn from the outcomes of their actions and adjust their behavior accordingly to optimize their performance.\n",
      "\n",
      "Let me know if you have any further questions or need clarification on any of these points!\n",
      "\n",
      "Sources used (3):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[1] Agentic AI Systems:\n",
      "    AI agents are autonomous systems that can:\n",
      "    - Perceive their environment through sensors or data\n",
      "    - Make decisions using...\n",
      "    From: ai_guide_4.txt\n",
      "    Section: 4\n",
      "\n",
      "[2] Agentic AI Systems:\n",
      "    AI agents are autonomous systems that can:\n",
      "    - Perceive their environment through sensors or data\n",
      "    - Make decisions using...\n",
      "    From: ai_guide_4.txt\n",
      "    Section: 4\n",
      "\n",
      "[3] Agentic AI Systems:\n",
      "    AI agents are autonomous systems that can:\n",
      "    - Perceive their environment through sensors or data\n",
      "    - Make decisions using...\n",
      "    From: ai_guide_4.txt\n",
      "    Section: 4\n",
      "\n",
      "================================================================================\n",
      "STEP 8: Streaming RAG Chain\n",
      "================================================================================\n",
      "\n",
      "Question: What is the purpose of chunking in RAG systems?\n",
      "--------------------------------------------------------------------------------\n",
      "Answer (streaming): Based on the context, it appears that the purpose of chunking is to split documents into smaller sections (chunks) to fit within the context window of a Large Language Model (LLM), thereby improving retrieval precision. This process involves considering key factors such as chunk size and overlap between chunks.\n",
      "\n",
      "However, I must note that the provided text does not explicitly mention \"RAG systems\" (I assume this refers to Retrieval-Augmented Generation systems). The text only discusses chunking strategies in general, without specifying a particular system or application. Therefore, while the information provided can be applied to RAG systems, it is not directly stated as such.\n",
      "\n",
      "If you'd like more information on RAG systems and how chunking applies to them specifically, I'd be happy to try and provide additional context!\n",
      "\n",
      "\n",
      "================================================================================\n",
      "STEP 9: Advanced Retrieval Options\n",
      "================================================================================\n",
      "Created MMR-based RAG chain for diverse results\n",
      "Created threshold-based RAG chain for high-quality matches only\n",
      "\n",
      "================================================================================\n",
      "Comparing Retrieval Strategies\n",
      "================================================================================\n",
      "\n",
      "Query: Tell me about embeddings\n",
      "\n",
      "1. Standard Similarity Search:\n",
      "--------------------------------------------------------------------------------\n",
      "1. - Instructor embeddings: Task-specific embeddings\n",
      "    The choice depends on accu...\n",
      "2. - Instructor embeddings: Task-specific embeddings\n",
      "    The choice depends on accu...\n",
      "3. - Instructor embeddings: Task-specific embeddings\n",
      "    The choice depends on accu...\n",
      "\n",
      "2. MMR Search (Diverse Results):\n",
      "--------------------------------------------------------------------------------\n",
      "1. - Instructor embeddings: Task-specific embeddings\n",
      "    The choice depends on accu...\n",
      "2. Embedding Models:\n",
      "    Embeddings are dense vector representations of text that c...\n",
      "3. Vector Databases and Embeddings:\n",
      "    Vector databases store information as numer...\n",
      "\n",
      "================================================================================\n",
      "RAG SYSTEM SUMMARY\n",
      "================================================================================\n",
      "\n",
      "✓ Knowledge Base: 5 documents\n",
      "✓ Chunks: 10 chunks\n",
      "✓ Embedding Model: sentence-transformers/all-MiniLM-L6-v2\n",
      "✓ Vector Store: ChromaDB (persistent)\n",
      "✓ Retrieval Methods: Similarity, MMR, Threshold\n",
      "✓ RAG Chain: Built with LCEL (modern approach)\n",
      "✓ Features: Streaming, Source Attribution, Multiple Retrieval Strategies\n",
      "\n",
      "Key LCEL Benefits:\n",
      "• More composable and flexible than old chains\n",
      "• Better streaming support for real-time responses\n",
      "• Easier to debug and modify components\n",
      "• Cleaner syntax with | operator\n",
      "• Built-in async support for performance\n",
      "\n",
      "Usage:\n",
      "1. Set GROQ_API_KEY in .env file (or use Ollama)\n",
      "2. Run: answer = rag_chain.invoke(\"Your question here\")\n",
      "3. For sources: result = rag_with_sources(\"Your question\")\n",
      "4. For streaming: rag_chain.stream(\"Your question\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Complete RAG (Retrieval-Augmented Generation) System\n",
    "Updated with LCEL (LangChain Expression Language) - Modern Approach\n",
    "\"\"\"\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def get_llm():\n",
    "    \"\"\"Initialize LLM based on environment configuration\"\"\"\n",
    "    llm_type = os.getenv(\"LLM_TYPE\", \"groq\")\n",
    "    if llm_type == \"groq\":\n",
    "        return ChatGroq(\n",
    "            model=\"llama-3.1-8b-instant\",\n",
    "            temperature=0.3,\n",
    "            max_tokens=500,\n",
    "            groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "        )\n",
    "    else:\n",
    "        return ChatOllama(\n",
    "            model=\"llama3.1\",\n",
    "            temperature=0.3\n",
    "        )\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 1: Prepare Documents and Create Knowledge Base\n",
    "# ==============================================================================\n",
    "\n",
    "# Sample knowledge base about AI concepts\n",
    "knowledge_base = [\n",
    "    \"\"\"\n",
    "    Vector Databases and Embeddings:\n",
    "    Vector databases store information as numerical vectors (embeddings) that \n",
    "    capture semantic meaning. When you search, your query is converted to a \n",
    "    vector and compared to stored vectors using similarity metrics like cosine \n",
    "    similarity. This enables semantic search - finding relevant information \n",
    "    based on meaning rather than exact keyword matches.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Retrieval-Augmented Generation (RAG):\n",
    "    RAG enhances Large Language Models by retrieving relevant information from \n",
    "    a knowledge base before generating responses. The process involves three steps:\n",
    "    1. Retrieve: Search for relevant documents using vector similarity\n",
    "    2. Augment: Add retrieved context to the prompt\n",
    "    3. Generate: The LLM creates an answer grounded in the retrieved information\n",
    "    This approach reduces hallucinations and provides up-to-date, specific knowledge.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Chunking Strategies:\n",
    "    Documents are split into chunks to fit within LLM context windows and improve \n",
    "    retrieval precision. Key considerations include:\n",
    "    - Chunk size: Smaller chunks (100-500 tokens) for precise retrieval\n",
    "    - Overlap: 10-20% overlap prevents losing context at boundaries\n",
    "    - Separators: Use natural boundaries (paragraphs, sentences)\n",
    "    - Metadata: Add source info, page numbers, timestamps for tracking\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Embedding Models:\n",
    "    Embeddings are dense vector representations of text that capture semantic meaning.\n",
    "    Popular models include:\n",
    "    - OpenAI text-embedding-ada-002: High quality, 1536 dimensions\n",
    "    - Sentence-BERT (all-MiniLM-L6-v2): Fast, efficient, 384 dimensions\n",
    "    - Instructor embeddings: Task-specific embeddings\n",
    "    The choice depends on accuracy needs, speed requirements, and cost constraints.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Agentic AI Systems:\n",
    "    AI agents are autonomous systems that can:\n",
    "    - Perceive their environment through sensors or data\n",
    "    - Make decisions using reasoning and planning\n",
    "    - Take actions using tools (APIs, databases, search engines)\n",
    "    - Learn from feedback to improve performance\n",
    "    Agents can use RAG to access knowledge dynamically during task execution.\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "# Create Document objects\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=text.strip(),\n",
    "        metadata={\"source\": f\"ai_guide_{i}.txt\", \"section\": i}\n",
    "    )\n",
    "    for i, text in enumerate(knowledge_base)\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 1: Document Preparation\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Created {len(documents)} documents\")\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 2: Split Documents into Chunks\n",
    "# ==============================================================================\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,      # Smaller chunks for more precise retrieval\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \"]\n",
    ")\n",
    "\n",
    "split_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 2: Document Chunking\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Split into {len(split_documents)} chunks\")\n",
    "for i, doc in enumerate(split_documents[:3]):\n",
    "    print(f\"\\nChunk {i+1}: {doc.page_content[:100]}...\")\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 3: Create Embeddings and Vector Store\n",
    "# ==============================================================================\n",
    "\"\"\"\n",
    "We use HuggingFace embeddings (free, runs locally)\n",
    "For production, consider: OpenAI embeddings (paid but higher quality)\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 3: Creating Vector Store\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Loading embedding model (this may take a moment)...\")\n",
    "\n",
    "# Initialize embedding model\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'},  # Use 'cuda' for GPU\n",
    "    encode_kwargs={'normalize_embeddings': True}  # Important for cosine similarity\n",
    ")\n",
    "\n",
    "# Create vector store from documents\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=split_documents,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"rag_knowledge_base\",\n",
    "    persist_directory=\"./chroma_rag_db\"  # Save to disk\n",
    ")\n",
    "\n",
    "print(\"Vector store created successfully!\")\n",
    "print(f\"Total vectors stored: {vectorstore._collection.count()}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 4: Test Retrieval\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 4: Testing Retrieval\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create a retriever with specific search parameters\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",  # or \"mmr\" for diverse results\n",
    "    search_kwargs={\"k\": 3}     # Return top 3 results\n",
    ")\n",
    "\n",
    "# Test query\n",
    "test_query = \"How does RAG work?\"\n",
    "retrieved_docs = retriever.invoke(test_query)\n",
    "\n",
    "print(f\"\\nQuery: '{test_query}'\")\n",
    "print(f\"Retrieved {len(retrieved_docs)} documents:\\n\")\n",
    "\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"{i+1}. {doc.page_content[:150]}...\")\n",
    "    print(f\"   Source: {doc.metadata.get('source', 'unknown')}\\n\")\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 5: Build RAG Chain Using LCEL (Modern Approach)\n",
    "# ==============================================================================\n",
    "\"\"\"\n",
    "LCEL (LangChain Expression Language) uses the | operator to chain components\n",
    "This replaces the deprecated RetrievalQA.from_chain_type() method\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 5: Building RAG Chain with LCEL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define a custom prompt template\n",
    "template = \"\"\"You are a helpful AI assistant. Use the following context to answer the question.\n",
    "If you cannot answer based on the context, say so.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: Let me help you understand this based on the provided information.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "print(\"\\nPrompt template created:\")\n",
    "print(template)\n",
    "\n",
    "# Helper function to format documents\n",
    "def format_docs(docs):\n",
    "    \"\"\"Format retrieved documents into a single context string\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Build the RAG chain using LCEL (pipe operator |)\n",
    "# This is the modern, recommended approach\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,  # Retrieve docs and format\n",
    "        \"question\": RunnablePassthrough()     # Pass question through\n",
    "    }\n",
    "    | prompt                                  # Format into prompt\n",
    "    | get_llm()                              # Send to LLM\n",
    "    | StrOutputParser()                      # Parse output to string\n",
    ")\n",
    "\n",
    "print(\"\\n✓ RAG chain created successfully using LCEL\")\n",
    "print(\"\\nChain structure:\")\n",
    "print(\"  Input → {context: retriever → format, question: passthrough}\")\n",
    "print(\"       → Prompt → LLM → Output Parser → Final Answer\")\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 6: Test the RAG Chain\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 6: Testing RAG Chain\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_questions = [\n",
    "    \"What is RAG?\",\n",
    "    \"What are the key considerations for chunking?\",\n",
    "    \"What embedding models are mentioned?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(\"-\" * 80)\n",
    "    try:\n",
    "        answer = rag_chain.invoke(question)\n",
    "        print(f\"Answer: {answer}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"Note: Make sure GROQ_API_KEY is set in your .env file\\n\")\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 7: RAG Chain with Source Documents\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 7: RAG Chain with Source Attribution\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def rag_with_sources(question: str):\n",
    "    \"\"\"\n",
    "    Enhanced RAG function that returns both answer and source documents\n",
    "    This allows you to cite sources for the generated answer\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve relevant documents\n",
    "    docs = retriever.invoke(question)\n",
    "    \n",
    "    # Step 2: Format context from documents\n",
    "    context = format_docs(docs)\n",
    "    \n",
    "    # Step 3: Create the formatted prompt\n",
    "    formatted_prompt = prompt.format(context=context, question=question)\n",
    "    \n",
    "    # Step 4: Get answer from LLM\n",
    "    llm = get_llm()\n",
    "    response = llm.invoke(formatted_prompt)\n",
    "    answer = response.content if hasattr(response, 'content') else str(response)\n",
    "    \n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"sources\": docs,\n",
    "        \"num_sources\": len(docs)\n",
    "    }\n",
    "\n",
    "# Test with source attribution\n",
    "test_question = \"Explain what AI agents can do\"\n",
    "print(f\"\\nQuestion: {test_question}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "try:\n",
    "    result = rag_with_sources(test_question)\n",
    "    \n",
    "    print(f\"\\nAnswer:\\n{result['answer']}\\n\")\n",
    "    print(f\"Sources used ({result['num_sources']}):\")\n",
    "    print(\"-\" * 80)\n",
    "    for i, doc in enumerate(result['sources'], 1):\n",
    "        print(f\"\\n[{i}] {doc.page_content[:150]}...\")\n",
    "        print(f\"    From: {doc.metadata.get('source', 'unknown')}\")\n",
    "        print(f\"    Section: {doc.metadata.get('section', 'N/A')}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Make sure GROQ_API_KEY is set in your .env file\")\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 8: Advanced RAG Chain with Streaming\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 8: Streaming RAG Chain\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Streaming allows you to see the response as it's generated\n",
    "def streaming_rag(question: str):\n",
    "    \"\"\"RAG with streaming output for better UX\"\"\"\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Answer (streaming): \", end=\"\", flush=True)\n",
    "    \n",
    "    try:\n",
    "        # Stream the response token by token\n",
    "        for chunk in rag_chain.stream(question):\n",
    "            print(chunk, end=\"\", flush=True)\n",
    "        print(\"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError: {e}\")\n",
    "\n",
    "# Test streaming\n",
    "streaming_rag(\"What is the purpose of chunking in RAG systems?\")\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 9: Advanced Retrieval Techniques\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 9: Advanced Retrieval Options\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Option 1: MMR (Maximal Marginal Relevance) - diverse results\n",
    "mmr_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\n",
    "        \"k\": 3,              # Number of documents to return\n",
    "        \"fetch_k\": 10,       # Number of candidates to consider\n",
    "        \"lambda_mult\": 0.5   # Diversity (0=max diversity, 1=max relevance)\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create MMR-based RAG chain\n",
    "rag_chain_mmr = (\n",
    "    {\"context\": mmr_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | get_llm()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"Created MMR-based RAG chain for diverse results\")\n",
    "\n",
    "# Option 2: Similarity threshold - only return docs above threshold\n",
    "threshold_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\n",
    "        \"score_threshold\": 0.7,  # Only return docs with similarity > 0.7\n",
    "        \"k\": 5\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create threshold-based RAG chain\n",
    "rag_chain_threshold = (\n",
    "    {\"context\": threshold_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | get_llm()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"Created threshold-based RAG chain for high-quality matches only\")\n",
    "\n",
    "# Test different retrieval strategies\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Comparing Retrieval Strategies\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "comparison_query = \"Tell me about embeddings\"\n",
    "\n",
    "print(f\"\\nQuery: {comparison_query}\\n\")\n",
    "\n",
    "# Standard similarity search\n",
    "print(\"1. Standard Similarity Search:\")\n",
    "print(\"-\" * 80)\n",
    "standard_docs = retriever.invoke(comparison_query)\n",
    "for i, doc in enumerate(standard_docs, 1):\n",
    "    print(f\"{i}. {doc.page_content[:80]}...\")\n",
    "\n",
    "# MMR search (more diverse)\n",
    "print(\"\\n2. MMR Search (Diverse Results):\")\n",
    "print(\"-\" * 80)\n",
    "mmr_docs = mmr_retriever.invoke(comparison_query)\n",
    "for i, doc in enumerate(mmr_docs, 1):\n",
    "    print(f\"{i}. {doc.page_content[:80]}...\")\n",
    "\n",
    "# ==============================================================================\n",
    "# Summary\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RAG SYSTEM SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\"\"\n",
    "✓ Knowledge Base: {len(documents)} documents\n",
    "✓ Chunks: {len(split_documents)} chunks\n",
    "✓ Embedding Model: sentence-transformers/all-MiniLM-L6-v2\n",
    "✓ Vector Store: ChromaDB (persistent)\n",
    "✓ Retrieval Methods: Similarity, MMR, Threshold\n",
    "✓ RAG Chain: Built with LCEL (modern approach)\n",
    "✓ Features: Streaming, Source Attribution, Multiple Retrieval Strategies\n",
    "\n",
    "Key LCEL Benefits:\n",
    "• More composable and flexible than old chains\n",
    "• Better streaming support for real-time responses\n",
    "• Easier to debug and modify components\n",
    "• Cleaner syntax with | operator\n",
    "• Built-in async support for performance\n",
    "\n",
    "Usage:\n",
    "1. Set GROQ_API_KEY in .env file (or use Ollama)\n",
    "2. Run: answer = rag_chain.invoke(\"Your question here\")\n",
    "3. For sources: result = rag_with_sources(\"Your question\")\n",
    "4. For streaming: rag_chain.stream(\"Your question\")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31326618",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.chains'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 11\u001b[39m\n",
      "\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocuments\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Document\n",
      "\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PromptTemplate\n",
      "\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchains\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RetrievalQA\n",
      "\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n",
      "\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n",
      "\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain.chains'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Complete RAG (Retrieval-Augmented Generation) System\n",
    "This demonstrates a full pipeline from documents to AI-generated answers\n",
    "\"\"\"\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def get_llm():\n",
    "    # We use Groq for logic/routing as it follows instructions well\n",
    "    llm_type = os.getenv(\"LLM_TYPE\", \"groq\")\n",
    "    if llm_type == \"groq\":\n",
    "        return ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.3, max_tokens=500)\n",
    "    else:\n",
    "        return ChatOllama(model=\"llama3.1\", temperature=0.3, max_tokens=500)\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 1: Prepare Documents and Create Knowledge Base\n",
    "# ==============================================================================\n",
    "\n",
    "# Sample knowledge base about AI concepts\n",
    "knowledge_base = [\n",
    "    \"\"\"\n",
    "    Vector Databases and Embeddings:\n",
    "    Vector databases store information as numerical vectors (embeddings) that \n",
    "    capture semantic meaning. When you search, your query is converted to a \n",
    "    vector and compared to stored vectors using similarity metrics like cosine \n",
    "    similarity. This enables semantic search - finding relevant information \n",
    "    based on meaning rather than exact keyword matches.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Retrieval-Augmented Generation (RAG):\n",
    "    RAG enhances Large Language Models by retrieving relevant information from \n",
    "    a knowledge base before generating responses. The process involves three steps:\n",
    "    1. Retrieve: Search for relevant documents using vector similarity\n",
    "    2. Augment: Add retrieved context to the prompt\n",
    "    3. Generate: The LLM creates an answer grounded in the retrieved information\n",
    "    This approach reduces hallucinations and provides up-to-date, specific knowledge.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Chunking Strategies:\n",
    "    Documents are split into chunks to fit within LLM context windows and improve \n",
    "    retrieval precision. Key considerations include:\n",
    "    - Chunk size: Smaller chunks (100-500 tokens) for precise retrieval\n",
    "    - Overlap: 10-20% overlap prevents losing context at boundaries\n",
    "    - Separators: Use natural boundaries (paragraphs, sentences)\n",
    "    - Metadata: Add source info, page numbers, timestamps for tracking\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Embedding Models:\n",
    "    Embeddings are dense vector representations of text that capture semantic meaning.\n",
    "    Popular models include:\n",
    "    - OpenAI text-embedding-ada-002: High quality, 1536 dimensions\n",
    "    - Sentence-BERT (all-MiniLM-L6-v2): Fast, efficient, 384 dimensions\n",
    "    - Instructor embeddings: Task-specific embeddings\n",
    "    The choice depends on accuracy needs, speed requirements, and cost constraints.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Agentic AI Systems:\n",
    "    AI agents are autonomous systems that can:\n",
    "    - Perceive their environment through sensors or data\n",
    "    - Make decisions using reasoning and planning\n",
    "    - Take actions using tools (APIs, databases, search engines)\n",
    "    - Learn from feedback to improve performance\n",
    "    Agents can use RAG to access knowledge dynamically during task execution.\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "# Create Document objects\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=text.strip(),\n",
    "        metadata={\"source\": f\"ai_guide_{i}.txt\", \"section\": i}\n",
    "    )\n",
    "    for i, text in enumerate(knowledge_base)\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 1: Document Preparation\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Created {len(documents)} documents\")\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 2: Split Documents into Chunks\n",
    "# ==============================================================================\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,      # Smaller chunks for more precise retrieval\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \"]\n",
    ")\n",
    "\n",
    "split_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 2: Document Chunking\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Split into {len(split_documents)} chunks\")\n",
    "for i, doc in enumerate(split_documents[:3]):\n",
    "    print(f\"\\nChunk {i+1}: {doc.page_content[:100]}...\")\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 3: Create Embeddings and Vector Store\n",
    "# ==============================================================================\n",
    "\"\"\"\n",
    "We use HuggingFace embeddings (free, runs locally)\n",
    "For production, consider: OpenAI embeddings (paid but higher quality)\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 3: Creating Vector Store\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Loading embedding model (this may take a moment)...\")\n",
    "\n",
    "# Initialize embedding model\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'},  # Use 'cuda' for GPU\n",
    "    encode_kwargs={'normalize_embeddings': True}  # Important for cosine similarity\n",
    ")\n",
    "\n",
    "# Create vector store from documents\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=split_documents,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"rag_knowledge_base\",\n",
    "    persist_directory=\"./chroma_rag_db\"  # Save to disk\n",
    ")\n",
    "\n",
    "print(\"Vector store created successfully!\")\n",
    "print(f\"Total vectors stored: {vectorstore._collection.count()}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 4: Test Retrieval\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 4: Testing Retrieval\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create a retriever with specific search parameters\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",  # or \"mmr\" for diverse results\n",
    "    search_kwargs={\"k\": 3}     # Return top 3 results\n",
    ")\n",
    "\n",
    "# Test query\n",
    "test_query = \"How does RAG work?\"\n",
    "retrieved_docs = retriever.invoke(test_query)\n",
    "\n",
    "print(f\"\\nQuery: '{test_query}'\")\n",
    "print(f\"Retrieved {len(retrieved_docs)} documents:\\n\")\n",
    "\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"{i+1}. {doc.page_content[:150]}...\")\n",
    "    print(f\"   Source: {doc.metadata.get('source', 'unknown')}\\n\")\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 5: Build RAG Chain (Without LLM for demonstration)\n",
    "# ==============================================================================\n",
    "\"\"\"\n",
    "The complete RAG chain would look like this:\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 5: RAG Chain Architecture\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define a custom prompt template\n",
    "template = \"\"\"\n",
    "You are a helpful AI assistant. Use the following context to answer the question.\n",
    "If you cannot answer based on the context, say so.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: Let me help you understand this based on the provided information.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "print(\"\\nPrompt template created:\")\n",
    "print(prompt.template)\n",
    "\n",
    "# Note: This is the structure for when you have an LLM API key\n",
    "\n",
    "# Create RAG chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=get_llm(),\n",
    "    chain_type=\"stuff\",  # \"stuff\" puts all docs in context\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# Use the chain\n",
    "result = qa_chain({\"query\": \"What is RAG?\"})\n",
    "print(f\"Answer: {result['result']}\")\n",
    "print(f\"Sources: {result['source_documents']}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 6: Manual RAG Simulation (Without LLM)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 6: Manual RAG Process Demonstration\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def manual_rag(query: str, retriever, max_docs: int = 3):\n",
    "    \"\"\"\n",
    "    Demonstrates the RAG process without calling an actual LLM\n",
    "    Shows what context would be sent to the model\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve relevant documents\n",
    "    docs = retriever.get_relevant_documents(query)[:max_docs]\n",
    "    \n",
    "    # Step 2: Format context\n",
    "    context = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" \n",
    "                           for i, doc in enumerate(docs)])\n",
    "    \n",
    "    # Step 3: Create the full prompt\n",
    "    full_prompt = f\"\"\"\n",
    "Context Information:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Instructions: Based on the context above, provide a comprehensive answer.\n",
    "\"\"\"\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"retrieved_docs\": docs,\n",
    "        \"context\": context,\n",
    "        \"full_prompt\": full_prompt,\n",
    "        \"num_docs\": len(docs)\n",
    "    }\n",
    "\n",
    "# Test the manual RAG\n",
    "test_queries = [\n",
    "    \"What is the purpose of chunking in RAG?\",\n",
    "    \"Explain what agents can do\",\n",
    "    \"What embedding models are available?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    result = manual_rag(query, retriever)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Query: {result['query']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Retrieved {result['num_docs']} documents\")\n",
    "    print(\"\\nThis prompt would be sent to the LLM:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(result['full_prompt'][:500] + \"...\")\n",
    "    print()\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 7: Advanced Retrieval Techniques\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 7: Advanced Retrieval Options\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Option 1: MMR (Maximal Marginal Relevance) - diverse results\n",
    "mmr_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\n",
    "        \"k\": 3,              # Number of documents to return\n",
    "        \"fetch_k\": 10,       # Number of candidates to consider\n",
    "        \"lambda_mult\": 0.5   # Diversity (0=max diversity, 1=max relevance)\n",
    "    }\n",
    ")\n",
    "\n",
    "# Option 2: Similarity threshold - only return docs above threshold\n",
    "threshold_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\n",
    "        \"score_threshold\": 0.7,  # Only return docs with similarity > 0.7\n",
    "        \"k\": 5\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Created advanced retrievers:\")\n",
    "print(\"1. MMR retriever (for diverse results)\")\n",
    "print(\"2. Threshold retriever (for high-quality matches only)\")\n",
    "\n",
    "# ==============================================================================\n",
    "# Summary\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RAG SYSTEM SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\"\"\n",
    "✓ Knowledge Base: {len(documents)} documents\n",
    "✓ Chunks: {len(split_documents)} chunks\n",
    "✓ Embedding Model: sentence-transformers/all-MiniLM-L6-v2\n",
    "✓ Vector Store: ChromaDB (persistent)\n",
    "✓ Retrieval Methods: Similarity, MMR, Threshold\n",
    "\n",
    "To complete the RAG system:\n",
    "1. Add OpenAI API key\n",
    "2. Initialize ChatOpenAI\n",
    "3. Create RetrievalQA chain\n",
    "4. Query and get AI-generated answers with sources\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
