{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff46edbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Original text length: 817 characters\n",
      "Number of chunks created: 7\n",
      "================================================================================\n",
      "\n",
      "Chunk 1 (129 chars):\n",
      "--------------------------------------------------------------------------------\n",
      "Artificial Intelligence (AI) has transformed numerous industries. \n",
      "In healthcare, AI assists in diagnosis and treatment planning.\n",
      "\n",
      "Chunk 2 (70 chars):\n",
      "--------------------------------------------------------------------------------\n",
      "Machine learning algorithms analyze medical images to detect diseases.\n",
      "\n",
      "Chunk 3 (137 chars):\n",
      "--------------------------------------------------------------------------------\n",
      "In finance, AI powers fraud detection systems and algorithmic trading.\n",
      "Natural language processing enables chatbots for customer service.\n",
      "\n",
      "Chunk 4 (75 chars):\n",
      "--------------------------------------------------------------------------------\n",
      "Computer vision is used in autonomous vehicles to perceive the environment.\n",
      "\n",
      "Chunk 5 (140 chars):\n",
      "--------------------------------------------------------------------------------\n",
      "Deep learning, a subset of machine learning, uses neural networks.\n",
      "These networks consist of layers that process information hierarchically.\n",
      "\n",
      "Chunk 6 (73 chars):\n",
      "--------------------------------------------------------------------------------\n",
      "Training requires large datasets and significant computational resources.\n",
      "\n",
      "Chunk 7 (181 chars):\n",
      "--------------------------------------------------------------------------------\n",
      "The future of AI includes quantum computing integration.\n",
      "Ethical considerations around AI bias and privacy are crucial.\n",
      "Responsible AI development ensures fairness and transparency.\n",
      "\n",
      "================================================================================\n",
      "Split Documents with Metadata:\n",
      "================================================================================\n",
      "\n",
      "Document 1:\n",
      "Content: Python is a versatile programming language.\n",
      "Metadata: {'source': 'python_guide.txt', 'page': 1, 'topic': 'programming'}\n",
      "\n",
      "Document 2:\n",
      "Content: Machine learning models require training data.\n",
      "Metadata: {'source': 'ml_basics.txt', 'page': 1, 'topic': 'ML'}\n",
      "\n",
      "Document 3:\n",
      "Content: Vector databases enable semantic search.\n",
      "Metadata: {'source': 'databases.txt', 'page': 1, 'topic': 'databases'}\n",
      "\n",
      "================================================================================\n",
      "Loaded Document:\n",
      "================================================================================\n",
      "Number of documents: 1\n",
      "Content preview: \n",
      "Introduction to Vector Databases\n",
      "\n",
      "Vector databases store data as high-dimensional vectors.\n",
      "These vectors capture semantic meaning of text.\n",
      "They enable similarity search based on meaning, not keywords...\n",
      "Metadata: {'source': 'sample_doc.txt'}\n",
      "\n",
      "After splitting: 7 chunks\n",
      "\n",
      "================================================================================\n",
      "Comparison of Splitting Strategies:\n",
      "================================================================================\n",
      "Character-based splitter: 5 chunks\n",
      "Token-based splitter: 3 chunks\n",
      "\n",
      "================================================================================\n",
      "Documents with Rich Metadata:\n",
      "================================================================================\n",
      "\n",
      "Content: Artificial Intelligence (AI) has transformed numerous industries. \n",
      "In healthcare...\n",
      "Metadata: {'source': 'ai_overview.txt', 'chunk_id': 0, 'chunk_size': 129, 'created_at': '2024-01-15', 'doc_type': 'tutorial'}\n",
      "\n",
      "Content: Machine learning algorithms analyze medical images to detect diseases....\n",
      "Metadata: {'source': 'ai_overview.txt', 'chunk_id': 1, 'chunk_size': 70, 'created_at': '2024-01-15', 'doc_type': 'tutorial'}\n",
      "\n",
      "================================================================================\n",
      "Chunking tutorial complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Advanced Document Processing: Loading, Chunking, and Embedding\n",
    "This shows how to process real documents for RAG systems\n",
    "\"\"\"\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import os\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 1: Understanding Text Splitters\n",
    "# ==============================================================================\n",
    "\"\"\"\n",
    "Why chunk documents?\n",
    "1. LLMs have token limits - can't process entire books\n",
    "2. Smaller chunks = more precise retrieval\n",
    "3. Better semantic focus per chunk\n",
    "\"\"\"\n",
    "\n",
    "# Sample long document\n",
    "long_text = \"\"\"\n",
    "Artificial Intelligence (AI) has transformed numerous industries. \n",
    "In healthcare, AI assists in diagnosis and treatment planning. \n",
    "Machine learning algorithms analyze medical images to detect diseases.\n",
    "\n",
    "In finance, AI powers fraud detection systems and algorithmic trading.\n",
    "Natural language processing enables chatbots for customer service.\n",
    "Computer vision is used in autonomous vehicles to perceive the environment.\n",
    "\n",
    "Deep learning, a subset of machine learning, uses neural networks.\n",
    "These networks consist of layers that process information hierarchically.\n",
    "Training requires large datasets and significant computational resources.\n",
    "\n",
    "The future of AI includes quantum computing integration.\n",
    "Ethical considerations around AI bias and privacy are crucial.\n",
    "Responsible AI development ensures fairness and transparency.\n",
    "\"\"\"\n",
    "\n",
    "# RecursiveCharacterTextSplitter: Splits by paragraphs, then sentences, then words\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,        # Target size of each chunk (characters)\n",
    "    chunk_overlap=50,      # Overlap between chunks (prevents losing context)\n",
    "    length_function=len,   # Function to measure chunk size\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]  # Try these separators in order\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(long_text)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"Original text length: {len(long_text)} characters\")\n",
    "print(f\"Number of chunks created: {len(chunks)}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"\\nChunk {i+1} ({len(chunk)} chars):\")\n",
    "    print(\"-\" * 80)\n",
    "    print(chunk)\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 2: Working with LangChain Documents\n",
    "# ==============================================================================\n",
    "\"\"\"\n",
    "LangChain Documents have:\n",
    "- page_content: The actual text\n",
    "- metadata: Information about the document (source, page number, etc.)\n",
    "\"\"\"\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Create documents manually\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"Python is a versatile programming language.\",\n",
    "        metadata={\"source\": \"python_guide.txt\", \"page\": 1, \"topic\": \"programming\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Machine learning models require training data.\",\n",
    "        metadata={\"source\": \"ml_basics.txt\", \"page\": 1, \"topic\": \"ML\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Vector databases enable semantic search.\",\n",
    "        metadata={\"source\": \"databases.txt\", \"page\": 1, \"topic\": \"databases\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "# Split documents while preserving metadata\n",
    "doc_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "split_docs = doc_splitter.split_documents(documents)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Split Documents with Metadata:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, doc in enumerate(split_docs):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 3: Loading Documents from Files\n",
    "# ==============================================================================\n",
    "\"\"\"\n",
    "LangChain provides loaders for various file types:\n",
    "- TextLoader: Plain text files\n",
    "- PyPDFLoader: PDF files\n",
    "- CSVLoader: CSV files\n",
    "- UnstructuredLoader: HTML, Word docs, etc.\n",
    "\"\"\"\n",
    "\n",
    "# Example: Create a sample text file\n",
    "sample_file = \"sample_doc.txt\"\n",
    "with open(sample_file, \"w\") as f:\n",
    "    f.write(\"\"\"\n",
    "Introduction to Vector Databases\n",
    "\n",
    "Vector databases store data as high-dimensional vectors.\n",
    "These vectors capture semantic meaning of text.\n",
    "They enable similarity search based on meaning, not keywords.\n",
    "\n",
    "Applications include:\n",
    "- Semantic search engines\n",
    "- Recommendation systems\n",
    "- Question answering systems\n",
    "- Document retrieval for RAG\n",
    "\n",
    "Popular vector databases include ChromaDB, Pinecone, and Weaviate.\n",
    "\"\"\")\n",
    "\n",
    "# Load the file\n",
    "loader = TextLoader(sample_file)\n",
    "loaded_docs = loader.load()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Loaded Document:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Number of documents: {len(loaded_docs)}\")\n",
    "print(f\"Content preview: {loaded_docs[0].page_content[:200]}...\")\n",
    "print(f\"Metadata: {loaded_docs[0].metadata}\")\n",
    "\n",
    "# Split the loaded document\n",
    "split_loaded_docs = doc_splitter.split_documents(loaded_docs)\n",
    "print(f\"\\nAfter splitting: {len(split_loaded_docs)} chunks\")\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 4: Advanced Chunking Strategies\n",
    "# ==============================================================================\n",
    "\n",
    "# Strategy 1: Character-based splitter (what we've been using)\n",
    "char_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=30,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Strategy 2: Token-based splitter (better for LLM context windows)\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "token_splitter = TokenTextSplitter(\n",
    "    chunk_size=50,     # Number of tokens\n",
    "    chunk_overlap=10   # Token overlap\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Comparison of Splitting Strategies:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_text = \"AI is transforming industries. \" * 20\n",
    "\n",
    "char_chunks = char_splitter.split_text(test_text)\n",
    "token_chunks = token_splitter.split_text(test_text)\n",
    "\n",
    "print(f\"Character-based splitter: {len(char_chunks)} chunks\")\n",
    "print(f\"Token-based splitter: {len(token_chunks)} chunks\")\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 5: Custom Metadata and Chunk IDs\n",
    "# ==============================================================================\n",
    "\"\"\"\n",
    "Adding rich metadata helps with:\n",
    "- Filtering results\n",
    "- Source attribution\n",
    "- Debugging\n",
    "- Access control\n",
    "\"\"\"\n",
    "\n",
    "def create_documents_with_rich_metadata(texts, source_name):\n",
    "    \"\"\"Create documents with comprehensive metadata\"\"\"\n",
    "    docs = []\n",
    "    for i, text in enumerate(texts):\n",
    "        doc = Document(\n",
    "            page_content=text,\n",
    "            metadata={\n",
    "                \"source\": source_name,\n",
    "                \"chunk_id\": i,\n",
    "                \"chunk_size\": len(text),\n",
    "                \"created_at\": \"2024-01-15\",\n",
    "                \"doc_type\": \"tutorial\"\n",
    "            }\n",
    "        )\n",
    "        docs.append(doc)\n",
    "    return docs\n",
    "\n",
    "rich_docs = create_documents_with_rich_metadata(chunks, \"ai_overview.txt\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Documents with Rich Metadata:\")\n",
    "print(\"=\" * 80)\n",
    "for doc in rich_docs[:2]:  # Show first 2\n",
    "    print(f\"\\nContent: {doc.page_content[:80]}...\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "\n",
    "# Cleanup\n",
    "os.remove(sample_file)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Chunking tutorial complete!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
